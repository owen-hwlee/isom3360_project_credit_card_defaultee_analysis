{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# ISOM3360 Data Mining for Business Analytics\n",
    "## Group 23 Project Code - Credit Card Defaultee Analysis\n",
    "### Part 2.3 - Naive Bayes Classifier\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: LAM, Ho Chit  \n",
    "ITSC: hclamao  \n",
    "SID: 20607878\n",
    "\n",
    "Name: LEE, Ho Wan Owen  \n",
    "ITSC: hwolee  \n",
    "SID: 20604852\n",
    "\n",
    "Name: LEE, Wai Chung  \n",
    "ITSC: wcleeaj  \n",
    "SID: 20702733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow of this notebook (TBC)\n",
    "\n",
    "1. Explore features and characteristics of dataset\n",
    "2. Drop columns of low data quality (e.g. large amounts of empty values)\n",
    "3. Determine $k$ columns to keep in the dataset (feature selection)\n",
    "4. Perform one-hot encoding\n",
    "5. Split into training and testing sets\n",
    "6. Perform data cleaning\n",
    "   - Dealing with missing values\n",
    "7. Perform data standardization / normalization\n",
    "8. Export preprocessed data to .csv files at `./data_preprocessed/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Decision Tree\n",
    "we will use all the train data (891 examples) to construct the tree and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define independent variables / attirbutes / features\n",
    "features = ['Pclass','Age_zscore','SibSp','Parch','Fare_zscore','Sex_male','Embarked_Q','Embarked_S']\n",
    "# define one single target variable / label\n",
    "target = ['Survived']\n",
    "\n",
    "# get defined training dataset\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Split data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into 80% and 20%, put 20% in testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=3360)\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build a Tree based on 80% train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def create_model(**params):\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=3360, **params)\n",
    "\n",
    "    # train the model by fit in 80% training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Depth:\", model.get_depth())\n",
    "    print(\"Leaves:\", model.get_n_leaves())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model by using default hyperparameter values\n",
    "model = create_model()\n",
    "\n",
    "# get prediction for X_val\n",
    "pred_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function for simple tree visualization\n",
    "\n",
    "def simple_tree_vis(model):\n",
    "    plt.figure(figsize = (100,150))\n",
    "    tree.plot_tree(model,ax=None, fontsize=50)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "simple_tree_vis(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# function for fancy tree visualization\n",
    "\n",
    "def tree_vis(model):\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                      feature_names=features,  \n",
    "                      class_names=['Did not survive', 'Survived'],\n",
    "                      filled = True, rounded=True,  \n",
    "                      special_characters=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.render(\"titanic_decisiontree\")\n",
    "    return graph\n",
    "\n",
    "# uncomment the next line for graphical representation of the decision tree\n",
    "# tree_vis(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the model on 20% validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  <!-- - F1 score -->\n",
    "- Display confusion matrix\n",
    "- Plot curves:\n",
    "  - Precision-Recall curve\n",
    "  - ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \n",
    "    # training\n",
    "    print(\"---------- Evaluation ----------\\n\")\n",
    "    print(\"Evaluation: Training\")\n",
    "    preds = model.predict(X_train)\n",
    "\n",
    "    # output all metrics scores\n",
    "    print(\"\\tAccuracy:\", accuracy_score(y_train, preds, normalize=True, sample_weight=None))\n",
    "    # print(\"Precision:\", precision_score(truth, preds, sample_weight=None))\n",
    "    # print(\"Recall:\", recall_score(truth, preds, sample_weight=None))\n",
    "\n",
    "    # display confusion matrix\n",
    "    print(\"\\tConfusion matrix:\\n\", confusion_matrix(y_train, preds))\n",
    "    \n",
    "    # print classification report\n",
    "    print(\"\\tClassification report:\\n\", classification_report(y_train, preds))\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    print(\"Evaluation: Validation\")\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    # output all metrics scores\n",
    "    print(\"\\tAccuracy:\", accuracy_score(y_val, preds, normalize=True, sample_weight=None))\n",
    "    # print(\"Precision:\", precision_score(truth, preds, sample_weight=None))\n",
    "    # print(\"Recall:\", recall_score(truth, preds, sample_weight=None))\n",
    "\n",
    "    # display confusion matrix\n",
    "    print(\"\\tConfusion matrix:\\n\", confusion_matrix(y_val, preds))\n",
    "    \n",
    "    # print classification report\n",
    "    print(\"\\tClassification report:\\n\", classification_report(y_val, preds))\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the difference between training and validation accuracy is substantial and the training accuracy is extremely close to 100%, it is safe to conclude that severe overfitting occured in this model with default hyperparameters.  \n",
    "There are 3 methods to reduce overfitting:\n",
    "- Hyperparameter tuning (manual)\n",
    "- Cross validation\n",
    "- Hyperparameter tuning (via GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_model(max_depth=8)\n",
    "evaluate_model(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_leaf_nodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model(max_leaf_nodes=50)\n",
    "evaluate_model(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_split = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = create_model(min_samples_split=2)\n",
    "evaluate_model(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_leaf = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = create_model(min_samples_leaf=6)\n",
    "evaluate_model(model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_impurity_decrease = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = create_model(min_impurity_decrease=0.05)\n",
    "evaluate_model(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combination of hyperparameters above\n",
    "- max_depth = 8\n",
    "- max_leaf_nodes = 50\n",
    "- min_samples_split = 2\n",
    "- min_samples_leaf = 6\n",
    "<!-- - min_impurity_decrease = 0.1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = create_model(max_depth=8, \n",
    "                      max_leaf_nodes=50,\n",
    "                      min_samples_split=2,\n",
    "                      min_samples_leaf=6)\n",
    "evaluate_model(model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score_cv = cross_val_score(model6, X, y, cv=10)\n",
    "print(\"CV results:\", score_cv)\n",
    "print(\"Mean =\", score_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create try_grid\n",
    "try_grid = [{'max_depth': np.arange(3, 16),     # 3 to 15\n",
    "             'max_leaf_nodes': np.arange(1, 19)*5,      # 5, 10, 15, ..., 90\n",
    "             'min_samples_split': np.arange(2, 7),     # 2 - 6\n",
    "             'min_samples_leaf': np.arange(3, 10),      # 3 - 9\n",
    "             'min_impurity_decrease': np.linspace(0, 0.225, 8),}]        # 0, 0.025, 0.05, etc., 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create GridSearchCV object\n",
    "DTM = GridSearchCV(DecisionTreeClassifier(random_state=3360), param_grid=try_grid, cv=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM.fit(X, y)\n",
    "\n",
    "print(\"Best params:\", DTM.best_params_)\n",
    "print(\"Best score :\", DTM.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of best model\n",
    "best_model = create_model(**DTM.best_params_)\n",
    "\n",
    "evaluate_model(best_model)\n",
    "simple_tree_vis(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data file for prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for prediction results\n",
    "preds = pd.DataFrame(index=test_df.index, columns=['Survived'])\n",
    "\n",
    "# store prediction results of best model into dataframe\n",
    "preds['Survived'] = best_model.predict(test_df[features])\n",
    "\n",
    "# export to csv file\n",
    "preds.to_csv('prediction.csv')\n",
    "\n",
    "preds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and findings\n",
    "\n",
    "The results are fairly predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the end of Part 2.3 Naive Bayes Classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
